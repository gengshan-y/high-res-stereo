<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- Style from https://people.eecs.berkeley.edu/~janner/o2p2/ -->
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

  <script src="./files/head.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Hierarchical Deep Stereo Matching on High-resolution Images</title>
  <link rel="stylesheet" href="./files/font.css">
  <link rel="stylesheet" href="./files/main.css">

  <style type="text/css">
    .content {
      width: 900px;
      margin: 25px auto;
      border-radius: 20px;
    }

    .description {
      font-family: "Times";
      white-space: pre;
      text-align: left;
    }

    /**
 * Style sheet used by new LibX tooltip code
 */

    /* We insert a <div> with libx-tooltip style under the body.
 * This will inherit body's style - we can't afford to inherit undesirable 
 * styles and we must redefine what we need.  OTOH, some things, e.g.
 * font-size, might be ok to be inherited to stay within the page's tone.
 */
    .libx-tooltip {
      display: none;
      overflow: visible;
      padding: 5px;
      z-index: 100;
      background-color: #eee;
      color: #000;
      font-weight: normal;
      font-style: normal;
      text-align: left;
      border: 2px solid #666;
      border-radius: 5px;
      -webkit-border-radius: 5px;
      -moz-border-radius: 5px;
    }

    .libx-tooltip p {
      /* override default 1em margin to keep paragraphs inside a tooltip closer together. */
      margin: .2em;
    }
  </style>
  <style type="text/css">
    /**
 * Style sheet used by LibX autolinking code
 */
    .libx-autolink {}
  </style>
</head>

<body>

  <div class="outercontainer">
    <div class="container">

      <div class="content project_title" style="text-align: center">
        <h1>Hierarchical Deep Stereo Matching on High-resolution Images</h1>
        <big style="color:grey;">
          CVPR 2019
        </big>
        <p id="authors">
        <table align="center" style="width:60%; text-align:center; table-layout: fixed">
          <tr>
            <th><a href="https://gengshan-y.github.io/">Gengshan Yang<sup>1</sup></a></th>
            <th><a href="http://joshmanela.me/">Joshua Manela<sup>2</sup></a></th>
            <th>Michael Happold<sup>2</sup></a></th>
            <th><a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan<sup>1,2</sup></a></th>
          </tr>
        </table>
        <sup>1</sup>Robotics Institute, Carnegie Mellon University<br>
        <sup>2</sup>Argo AI
        </p>
      </div>

      <div class="content">

        <div class="content">
          <h3>Abstract</h3>
          <p>We explore the problem of real-time stereo matching on high-res imagery. Many state-of-the-art (SOTA)
            methods struggle to process high-res imagery because of memory constraints or fail to meet real-time needs.
            To address this issue, we propose an end-to-end framework that searches for correspondences incrementally
            over a coarse-to-fine hierarchy. Because high-res stereo datasets are relatively rare, we introduce a
            large-scale dataset of high-res stereo pairs for both training and evaluation. At the time of submission,
            our approach achieved SOTA performance on Middlebury-v3 and KITTI-15 while running significantly faster than
            its competitors. The hierarchical design also naturally allows for anytime on-demand reports of disparity by
            capping intermediate coarse results, allowing us to accurately predict disparity for near-range structures
            with low latency (30ms). We demonstrate that the performance-vs-speed tradeoff afforded by on-demand
            hierarchies may address sensing needs for time-critical applications such as autonomous driving.</p>
          <div id="teaser" style="margin: 12px; text-align: left;">
            <a href="http://arxiv.org/abs/1912.06704">[Paper]</a>
            <a href="https://github.com/gengshan-y/high-res-stereo">[Code and Data]</a>
            <a href="./2471_poster_v3.pdf">[Poster]</a>
            <a
              href="https://docs.google.com/presentation/d/1UIoZW30NjcSQZF9nXgfBduMSYyNu8IdjH86Dp-5VRWs/edit?usp=sharing">[Slides]</a>
            <a href="./deep-highres-stereo-sup-gengshan.pdf">[Supplement]</a>
            <a href="./cvpr19stereo.bib">[Bibtex]</a>
          </div>
        </div>



        <div class="content">
          <!--           <div class="text">
          </div> -->
          <h3>Results</h3>
          <div class="project_headline">
            <li>On-demand depth estimation on a coarse-to-fine hierarchy (<a
                href="http://www.youtube.com/embed/NPGnG4DhGNQ">high-res version on youtube</a>):</li>
            <iframe width="720" height="340" src="cvpr19-bicycle.gif" frameborder="0" allowfullscreen></iframe>
            <!--  <p>If you cannot access YouTube, please <a href="./">download our video here</a>.</p>-->
          </div>

          <br>
          <div class="project_headline">
            <li>Able to handle large view change in high-res photo (used as a submodule in <a
                href="https://www.youtube.com/watch?v=sq2hhkHgtb0">Open4D Bansal et al, CVPR 2020.)</a>:</li>
            <iframe width="960" height="510" src="cvpr19-dance.gif" frameborder="0" allowfullscreen></iframe>
          </div>
          <div class="project_headline">

            <br>
            <li>Results on high-res Middlebury dataset (<a href="http://www.youtube.com/embed/Oa-ZAfmbzUc">higher-res
                version on youtube</a>):</li>
            <iframe width="720" height="480" src="cvpr19-middlebury1.gif" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>

      </div>
    </div>


    <div class="content">
      <h2>Bibtex</h2>
      <p class="description">
        @inproceedings{yang2019hierarchical,
        title={Hierarchical deep stereo matching on high-resolution images},
        author={Yang, Gengshan and Manela, Joshua and Happold, Michael and Ramanan, Deva},
        booktitle={CVPR},
        pages={5515--5524},
        year={2019}
        }

    </div>

    <div class="content">
      <h2>Acknowledgments</h2>
      <p>This work was supported by the <a href="https://labs.ri.cmu.edu/argo-ai-center/">CMU Argo AI Center for
          Autonomous Vehicle Research</a>.</p>
    </div>

    <br><br><br><br>
  </div>
</body>

</html>